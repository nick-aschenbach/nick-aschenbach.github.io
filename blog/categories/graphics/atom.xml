<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Graphics | Aschenblog]]></title>
  <link href="http://nick-aschenbach.github.io/blog/categories/graphics/atom.xml" rel="self"/>
  <link href="http://nick-aschenbach.github.io/"/>
  <updated>2016-01-17T17:41:27-08:00</updated>
  <id>http://nick-aschenbach.github.io/</id>
  <author>
    <name><![CDATA[Nick Aschenbach]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Arkanoid Game Levels]]></title>
    <link href="http://nick-aschenbach.github.io/blog/2015/04/27/arkanoid-game-levels/"/>
    <updated>2015-04-27T19:12:09-07:00</updated>
    <id>http://nick-aschenbach.github.io/blog/2015/04/27/arkanoid-game-levels</id>
    <content type="html"><![CDATA[{% img left /assets/2015-04-27-arkanoid-game-levels/images/arkanoid_arcadeflyer.png auto auto %} I loved playing
Arkanoid when I was a kid. I spent hours playing the game over at my friend's house when I was growing up on
the original Nintendo Entertainment System. It was rated as one of the top games after its release in 1986.

I found a great Arkanoid background map set on [NES maps](http://www.nesmaps.com/) by Rick Bruns (see below). Each
level was perfectly aligned at 192 pixels wide by 232 pixels tall. The top, left and right edges were 8 pixels. Each
brick was 16 pixels wide by 8 pixels tall. With a little math (and confirming in Photoshop), I found that the
background could support exactly 11 brick tiles wide by 28 tall.

This made the map set the perfect asset for image parsing. My plan was to take the image as input and generate
source code with level data as output. Finally, I used a 3D graphics library to render the levels in browser.

**Image Parsing**

I wanted to use the [Chunky PNG](https://github.com/wvanbergen/chunky_png) and
[Oily PNG](https://github.com/wvanbergen/oily_png) gems for image parsing. The former is a 100% Ruby implementation
for encoding and decoding PNG images with read / write access at the pixel level. The latter gem uses native C
extensions to improve encoding and decoding speed for Chunky. I used these gems on other projects with good success.

{% img /assets/2015-04-27-arkanoid-game-levels/images/arkanoid.png auto auto %}

<!-- more -->

The brick colors were consistent among all of the levels. Rick created a legend with all of the colors on one page for
convenience. I created a hash in Ruby to associate R, G, B values with a color index using Photoshop and my IDE:

{% codeblock lang:rb %}
COLOR_MAP = {
  &#8220;252,252,252&#8221; => 1, # white
  &#8220;252,116,96&#8221; => 2, # orange
  &#8220;60,188,252&#8221; => 3, # light blue
  &#8220;128,208,16&#8221; => 4, # green
  &#8220;216,40,0&#8221; => 5, # red
  &#8220;0,112,236&#8221; => 6, # blue
  &#8220;252,116,180&#8221; => 7, # pink
  &#8220;252,152,56&#8221; => 8, # yellow
  &#8220;188,188,188&#8221; => 9, # silver
  &#8220;240,188,60&#8221; => 10 # gold
}
{% endcodeblock %}

I mentioned earlier that bricks were 16 pixels x 8 pixels, however some tiles had a drop shadow of one or two pixels.
I decided to scan images for blocks of color that matched one of the keys in the hash above. I made this decision
because I needed to be able to differentiate between the background (which often had long runs of pixels) and bricks.

I stored the color of the pixel in the upper left corner of a block and checked each color against this. If a
sufficiently large block of color was found, I determined that it was a brick and returned `nil` otherwise:

{% codeblock lang:rb %}
def get_brick_color(image, x, y)
  r_initial = ChunkyPNG::Color.r(image[x, y])
  g_initial = ChunkyPNG::Color.g(image[x, y])
  b_initial = ChunkyPNG::Color.b(image[x, y])

  (0&#8230;14).each do |x_offset|
    (0&#8230;6).each do |y_offset|
      r = ChunkyPNG::Color.r(image[x + x_offset, y + y_offset])
      g = ChunkyPNG::Color.g(image[x + x_offset, y + y_offset])
      b = ChunkyPNG::Color.b(image[x + x_offset, y + y_offset])

      return nil if r != r_initial || g != g_initial || b != b_initial
    end
  end

  return r_initial, g_initial, b_initial
end
{% endcodeblock %}

My plan was to scan the 11 tile x 28 tile grid for bricks using the `get_brick_color` method. Finally,
I wrote a little code to generate Javascript code by printing to standard output. I decided to output two digit numbers
so that the grid index colors would line up because the color indexes went up to 10.

Here is the full source for the image parsing code:

{% codeblock lang:rb %}
require &#8216;oily_png&#8217;
require &#8216;awesome_print&#8217;

COLOR_MAP = {
  &#8220;252,252,252&#8221; => 1, # white
  &#8220;252,116,96&#8221; => 2, # orange
  &#8220;60,188,252&#8221; => 3, # light blue
  &#8220;128,208,16&#8221; => 4, # green
  &#8220;216,40,0&#8221; => 5, # red
  &#8220;0,112,236&#8221; => 6, # blue
  &#8220;252,116,180&#8221; => 7, # pink
  &#8220;252,152,56&#8221; => 8, # yellow
  &#8220;188,188,188&#8221; => 9, # silver
  &#8220;240,188,60&#8221; => 10 # gold
}

# Notes:
# 8 pixel border left, top and right
# brick size: 16 wide x 8 tall (including 1 px shadow)
# Each sub image size is 192, 232
def scan_grid_location(image, sheet_grid_x, sheet_grid_y)
  startx = sheet_grid_x * 192
  starty = sheet_grid_y * 232

  array = initialize_array
  # 11 columns wide x 28 rows tall
  (0..11).each do |brick_x|
    (0..28).each do |brick_y|
      color = get_brick_color(image, startx + brick_x * 16 + 8, starty + brick_y * 8 + 8)
      next if color.nil?

      color = &#8220;#{color[0]},#{color[1]},#{color[2]}&#8221;
      color_index = COLOR_MAP[color]
      raise StandardError if color_index.nil?

      array[brick_x][brick_y] = color_index
    end
  end

  array
end

# Zero out the brick array
def initialize_array
  array = []
  11.times do
    inner_array = []
    28.times do
      inner_array.push(0)
    end
    array.push(inner_array)
  end
  array
end

# Scan to ensure block is a brick
def get_brick_color(image, x, y)
  r_initial = ChunkyPNG::Color.r(image[x, y])
  g_initial = ChunkyPNG::Color.g(image[x, y])
  b_initial = ChunkyPNG::Color.b(image[x, y])

  (0&#8230;14).each do |x_offset|
    (0&#8230;6).each do |y_offset|
      r = ChunkyPNG::Color.r(image[x + x_offset, y + y_offset])
      g = ChunkyPNG::Color.g(image[x + x_offset, y + y_offset])
      b = ChunkyPNG::Color.b(image[x + x_offset, y + y_offset])

      return nil if r != r_initial || g != g_initial || b != b_initial
    end
  end

  return r_initial, g_initial, b_initial
end

arkanoid = ChunkyPNG::Image.from_file(&#8216;arkanoid.png&#8217;)

# Emit javascript code
puts &#8216;[&#8217;
(0..6).each do |y|
  (0..4).each do |x|
    level = scan_grid_location(arkanoid, x, y)

    puts &#8216;[&#8217;
    (0&#8230;28).each do |brick_y|
      print &#8216;[&#8217;
      (0&#8230;11).each do |brick_x|
        print level[brick_x][brick_y].to_s.rjust(2, &#8216;0&#8217;) + &#8216;, &#8217;
      end
      puts &#8216;],&#8217;
    end
    puts &#8216;],&#8217;
  end
end
puts &#8216;];&#8217;
{% endcodeblock %}

Here is sample output from the second map file:

{% codeblock lang:js %}
[
  [00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00],
  [00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00],
  [02, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00],
  [02, 03, 00, 00, 00, 00, 00, 00, 00, 00, 00],
  [02, 03, 04, 00, 00, 00, 00, 00, 00, 00, 00],
  [02, 03, 04, 06, 00, 00, 00, 00, 00, 00, 00],
  [02, 03, 04, 06, 05, 00, 00, 00, 00, 00, 00],
  [02, 03, 04, 06, 05, 02, 00, 00, 00, 00, 00],
  [02, 03, 04, 06, 05, 02, 03, 00, 00, 00, 00],
  [02, 03, 04, 06, 05, 02, 03, 04, 00, 00, 00],
  [02, 03, 04, 06, 05, 02, 03, 04, 06, 00, 00],
  [02, 03, 04, 06, 05, 02, 03, 04, 06, 05, 00],
  [09, 09, 09, 09, 09, 09, 09, 09, 09, 09, 02],
  [00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00],
  [00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00],
  [00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00],
  [00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00],
  [00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00],
  [00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00],
  [00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00],
  [00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00],
  [00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00],
  [00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00],
  [00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00],
  [00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00],
  [00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00],
  [00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00],
  [00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00],
],
{% endcodeblock %}

** Visualization **

I use the [Three.js](http://threejs.org/) Javascript library for in browser 3D graphics projects. It abstracts
some of the low level details of WebGL and provides some nice primitives.

I started with some boilerplate code which I modified from Three.js:

{% codeblock lang:js %}
// Define scene, camera and renderer
var scene = new THREE.Scene();
var camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);
camera.position.z = 30;
var renderer = new THREE.WebGLRenderer({antialias: true});

renderer.setSize(window.innerWidth, window.innerHeight);
document.body.appendChild(renderer.domElement);

// Render frames in a tight loop
var render = function() {
  requestAnimationFrame(render);
  renderer.render(scene, camera);
};

render();
{% endcodeblock %}


I defined some of the basics: a `scene`, `camera` and a `renderer`. The `render` function creates a loop that
triggers the scene to be redrawn at 60 frames per second.

I divided the drawing elements into three parts: drawing the level, drawing a wireframe box to enclose the level and
lights that provide some nice effects.

Here is the source code that describes drawing the level:

{% codeblock lang:js %}
// Draw bricks (defined in levels.js)
var draw_level = function(scene) {
  for (var i = 0; i < 28; i++) {
    for (var j = 0; j < 11; j++) {
      var color_index = level_data[level][i][j];
      if (color_index == 0) continue;

      var shininess = 10;
      if (color_index >= 9) shininess = 50; // Gold and silver

      var material = new THREE.MeshPhongMaterial({
        color: color_data[color_index],
        specular: 0x232322,
        shininess: 50
      });
      var geometry = new THREE.BoxGeometry(2, 1, 0.5);
      mesh = new THREE.Mesh(geometry, material);
      mesh.position.z = 1.5;
      mesh.position.x += j * 2.2 - 12;
      mesh.position.y += i * -1.2 + 18;
      scene.add(mesh);
      object_list.push(mesh);
    }
  }
};
{% endcodeblock %}

I defined a `level_data` array (see `level.js` below). This code was the output from the Ruby image parser described
earlier in the post. We look up the `color_index` based on where are are in the loop. If the index is zero, then we skip
that element. Otherwise, we create a new box that is twice is wide as it is tall (proportional to the 16 pixel x 8
pixel blocks from the original image).

We assign a material using Phong shading and the color defined in a `color_data` array:

{% codeblock lang:js %}
color_data = [
  0x000000,
  0xfcfcfc,
  0xfc7460,
  0x3cbcfc,
  0x80d010,
  0xd82800,
  0x0070ec,
  0xfc74b4,
  0xfc9838,
  0xbcbcbc,
  0xf0bc3c
];
{% endcodeblock %}

The index of the array corresponds to the value in the `level_data` array. We use black `0x000000` as a placeholder
color. Finally we define a mesh with the `geometry` and `material` variables. The brick position is defined by
its (i, j) position.

Here is the demo for the code ([fullscreen](/assets/2015-04-27-arkanoid-game-levels/demo/index.html)):

<iframe src="http://nick-aschenbach.github.io/assets/2015-04-27-arkanoid-game-levels/demo/index.html" width="700" height="600"></iframe>

These are the relevant full source files:

- [3d.js](/assets/2015-04-27-arkanoid-game-levels/demo/js/3d.js) - Event handling and rendering code
- [levels.js](/assets/2015-04-27-arkanoid-game-levels/demo/js/levels.js) - Level and color data]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Simple Isometric Tiling]]></title>
    <link href="http://nick-aschenbach.github.io/blog/2015/02/25/isometric-tile-engine/"/>
    <updated>2015-02-25T22:40:30-08:00</updated>
    <id>http://nick-aschenbach.github.io/blog/2015/02/25/isometric-tile-engine</id>
    <content type="html"><![CDATA[In this post I share a few simple javascript examples of isometric tiling commonly used to create the illusion
of 3D graphics in games.

For years, side-scrolling video games use a projection where the camera is aligned along one of the axes. Many original
game titles like Tetris, Zelda, Super Mario Bros place the camera either above the player looking down along a
vertical axis or looking directly from the side. Basic techniques like shading and parallax scrolling (where foreground
images scroll faster than background images) help to provide a sense of depth.

{% img /assets/2015-02-25-isometric-tile-engine/images/mario.gif auto 300 %}

An [isometric projection](http://en.wikipedia.org/wiki/Isometric_projection) is a popular way of visualizing 3D objects
on a 2D screen. This involves rotating the camera 45 degrees to one side and then angling down roughly 30 degrees. This
approach is used in several role playing and strategy games (Sim City 2000 pictured below). Q*bert, released in 1982,
was perhaps one of the first games that used isometric graphics.

{% img /assets/2015-02-25-isometric-tile-engine/images/simcity.png auto 300 %}

<!-- more -->

**Drawing the Grid**

A common pattern is to use tiles that are two times wider than they are tall. Also, I find that lines with a slope of 2:1
(two pixels horizontally : one vertical) look better as pixel art. This ratio makes it relatively easy to calculate
the screen position of a tile and to find the position of the mouse over a tile.

Here is a simple example with 128x64 tiles on a HTML5 canvas:

<iframe src="http://nick-aschenbach.github.io/assets/2015-02-25-isometric-tile-engine/isometric01/index.html" width="100%" height="375"></iframe>

This snippet describes how to position the diamond / rhombus shapes in a grid
([full source](/assets/2015-02-25-isometric-tile-engine/isometric01/js/isometric.js)):

{% codeblock lang:js %}
tileColumnOffset: 64,
tileRowOffset: 32,

redrawTiles: function() {
  for(var Xi = 0; Xi < this.Xtiles; Xi++) {
    for(var Yi = 0; Yi < this.Ytiles; Yi++) {
      var offX = Xi * this.tileColumnOffset / 2 + Yi * this.tileColumnOffset / 2 + this.originX;
      var offY = Yi * this.tileRowOffset / 2 - Xi * this.tileRowOffset / 2 + this.originY;

      // Draw tile outline
      var color = '#999';
      this.drawLine(offX, offY + this.tileRowOffset / 2, offX + this.tileColumnOffset / 2, offY, color);
      this.drawLine(offX + this.tileColumnOffset / 2, offY, offX + this.tileColumnOffset, offY + this.tileRowOffset / 2, color);
      this.drawLine(offX + this.tileColumnOffset, offY + this.tileRowOffset / 2, offX + this.tileColumnOffset / 2, offY + this.tileRowOffset, color);
      this.drawLine(offX + this.tileColumnOffset / 2, offY + this.tileRowOffset, offX, offY + this.tileRowOffset / 2, color);
    }
  }
}
{% endcodeblock %}

When we use actual graphical tiles `redrawTiles`, the draw order will need to be changed so that tiles in the back
are rendered before ones in the front. Let's extend the work by adding colors, coordinates and some mouse listeners:

<iframe src="http://nick-aschenbach.github.io/assets/2015-02-25-isometric-tile-engine/isometric02/index.html" width="100%" height="375"></iframe>

Here is the salient code that handles canvas drawing
([full source](/assets/2015-02-25-isometric-tile-engine/isometric02/js/isometric.js)):

{% codeblock lang:js %}
redrawTiles: function() {
  for(var Xi = (this.Xtiles - 1); Xi >= 0; Xi&#8211;) {
    for(var Yi = 0; Yi < this.Ytiles; Yi++) {
      this.drawTile(Xi, Yi);
    }
  }
},

drawTile: function(Xi, Yi) {
  var offX = Xi * this.tileColumnOffset / 2 + Yi * this.tileColumnOffset / 2 + this.originX;
  var offY = Yi * this.tileRowOffset / 2 - Xi * this.tileRowOffset / 2 + this.originY;

  // Draw tile interior
  if( Xi == this.selectedTileX && Yi == this.selectedTileY)
    this.context.fillStyle = 'yellow';
  else
    this.context.fillStyle = 'green';
  this.context.moveTo(offX, offY + this.tileRowOffset / 2);
  this.context.lineTo(offX + this.tileColumnOffset / 2, offY, offX + this.tileColumnOffset, offY + this.tileRowOffset / 2);
  this.context.lineTo(offX + this.tileColumnOffset, offY + this.tileRowOffset / 2, offX + this.tileColumnOffset / 2, offY + this.tileRowOffset);
  this.context.lineTo(offX + this.tileColumnOffset / 2, offY + this.tileRowOffset, offX, offY + this.tileRowOffset / 2);
  this.context.stroke();
  this.context.fill();
  this.context.closePath();

  // Draw tile outline
  var color = '#999';
  this.drawLine(offX, offY + this.tileRowOffset / 2, offX + this.tileColumnOffset / 2, offY, color);
  this.drawLine(offX + this.tileColumnOffset / 2, offY, offX + this.tileColumnOffset, offY + this.tileRowOffset / 2, color);
  this.drawLine(offX + this.tileColumnOffset, offY + this.tileRowOffset / 2, offX + this.tileColumnOffset / 2, offY + this.tileRowOffset, color);
  this.drawLine(offX + this.tileColumnOffset / 2, offY + this.tileRowOffset, offX, offY + this.tileRowOffset / 2, color);

  if(this.showCoordinates) {
    this.context.fillStyle = 'orange';
    this.context.fillText(Xi + ", " + Yi, offX + this.tileColumnOffset/2 - 9, offY + this.tileRowOffset/2 + 3);
  }
},
{% endcodeblock %}

Determining the location of the mouse over a canvas tile (tileX, tileY) is determined by a few calculations in a
mouse move listener:

{% codeblock lang:js %}
$(window).on('mousemove', function(e) {
  e.pageX = e.pageX - self.tileColumnOffset / 2 - self.originX;
  e.pageY = e.pageY - self.tileRowOffset / 2 - self.originY;
  tileX = Math.round(e.pageX / self.tileColumnOffset - e.pageY / self.tileRowOffset);
  tileY = Math.round(e.pageX / self.tileColumnOffset + e.pageY / self.tileRowOffset);
{% endcodeblock %}

**Graphical Tiles**

[Open Game Art](http://opengameart.org/content/isometric-tiles) has over a hundred free tile sets worth investigating.
However, [Kenny.nl](http://www.kenney.nl/assets) provides a handful of professional looking isometric tile sets. The
quality is just excellent. He provides both individual isometric tiles and sprite sheets. XML metadata is also provided
that indicates tile location and size in the sprite sheets.

<iframe src="http://nick-aschenbach.github.io/assets/2015-02-25-isometric-tile-engine/isometric03/index.html" width="100%" height="400px"></iframe>

The code is very similar between the last two examples. A few small tweaks are needed to ensure that
tile height is consistent. Also, using tile sprite sheets would result in a performance enhancement due to the
fact that each image is loaded individually with the current implementation. However, the goal here was to keep things
as simple as possible.

The main Javascript files are:

- `isometric.js` which initializes the map and handles rendering and event handling
- `map.js` which stores a 2D array of map data and an array of image locations.

The source code and images for this are available on
[Github](https://github.com/nick-aschenbach/simple-isometric-tile-engine).

**Additional Resources**

- [Drawing Isometric Game Worlds](http://stackoverflow.com/questions/892811/drawing-isometric-game-worlds) has some
pointers on the math and layout.
- [Isometric Primer for Game Developers](http://gamedevelopment.tutsplus.com/tutorials/creating-isometric-worlds-a-primer-for-game-developers&#8211;gamedev-6511)
provides a good foundation and covers bounds detection, depth sorting and animation
- [Overviewer](https://github.com/overviewer/Minecraft-Overviewer), a tool for mapping Minecraft worlds using Google
Maps has excellent [design documentation](http://docs.overviewer.org/en/latest/design/designdoc/) that covers its
sprite rendering engine.]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Building Better Worlds With Terragen 3]]></title>
    <link href="http://nick-aschenbach.github.io/blog/2015/01/12/building-better-worlds/"/>
    <updated>2015-01-12T21:13:04-08:00</updated>
    <id>http://nick-aschenbach.github.io/blog/2015/01/12/building-better-worlds</id>
    <content type="html"><![CDATA[For years I have been interested in modeling terrain. I wrote 2D and 3D fractal terrain generators for fun when I was in school. To help inspire a side project I am working on, I utilized an excellent rendering engine for natural environments and terrain called [Terragen 3](http://planetside.co.uk/products/terragen3) from Planetside Software. Here are a couple of screen shots from their [image gallery](http://planetside.co.uk/galleries/terragen-gallery):

[{% img /assets/2015-01-12-building-better-worlds/images/lake.jpg 300 auto %}](/assets/2015-01-12-building-better-worlds/images/lake.jpg)
[{% img /assets/2015-01-12-building-better-worlds/images/desert.jpg 300 auto %}](/assets/2015-01-12-building-better-worlds/images/desert.jpg)
[{% img /assets/2015-01-12-building-better-worlds/images/village.jpg 300 auto %}](/assets/2015-01-12-building-better-worlds/images/village.jpg)
[{% img /assets/2015-01-12-building-better-worlds/images/volcano.jpg 300 auto %}](/assets/2015-01-12-building-better-worlds/images/volcano.jpg)

<!-- more -->

I found the [free version](http://planetside.co.uk/products/download-tg3) fairly capable. There are some limitations over the pay version of the software that include restricting resolution, render detail and degree to which anti-aliasing is applied. However, what impressed me most is how easy it is to get started (user interface pictured below left).

When you load the program it creates a simple environment for you. To render this basic scene, click the `View->Render` menu. Click `Render` button and wait. On my Macbook Pro, the rendering took approximately 40 seconds (pictured below right). A shortcut for the Render command is ⌘R. There is a rough view in the upper right that shows an approximation of how the rendered scene will appear.

[{% img /assets/2015-01-12-building-better-worlds/images/ui.png 300 auto %}](/assets/2015-01-12-building-better-worlds/images/ui.png)
[{% img /assets/2015-01-12-building-better-worlds/images/basic.jpg 300 auto %}](/assets/2015-01-12-building-better-worlds/images/basic.jpg)

The node network displayed in the lower right part of the interface is a hierarchial display of the scene. This includes Renderers, Cameras and scene elements like clouds, terrain and shaders. Double clicking on the `Render 01` node in the `Renderers` pane brings up the properties (to change the resolution, etc). This is limited to a maximum of 1280x900 in the free version.

The same configuration page is also available via the `Renderers` button on the bar at the top of the window. Adding new elements is easy. For example, to add clouds to the scene click `Atmosphere` and then the `Add cloud layer` button. I find the 3D / Volumetric clouds to be more interesting than the 2D ones.

I spent some time following Vladimir Chopine&#8217;s video tutorials on youtube. This was a good starting point for me:

{% youtube HnRFJ4Vptt8 %}

I followed along with a couple of videos and with some trial and error created this rendering:

[{% img left /assets/2015-01-12-building-better-worlds/images/mountain.jpg auto auto %}](/assets/2015-01-12-building-better-worlds/images/mountain.jpg)

Here is the Terragen 3 [file](/assets//2015-01-12-building-better-worlds/images/layered_colors3.tgd) I used to generate the above rendering. Mainly I focused on creating shaders for the elements in the scene (grass, snow, sand). I played with the atmospheric, lighting and cloud elements until I found something that I liked. The rendering took about 11 minutes to complete on my laptop.

Terragen supports import and export FBX, Wavefront OBJ, Lightwave LWO2 and their own native TGO format. It can also import and export heightfield / DEM formats including real geographical data from GeoTIFFs. This makes it easy to communicate with other modeling packages like Rhino, Maya, 3D Studio Max, Blender and Cinema 4D.
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Generating Code From Images of Color Gradients]]></title>
    <link href="http://nick-aschenbach.github.io/blog/2014/07/18/generating-code-from-color-gradients/"/>
    <updated>2014-07-18T19:27:15-07:00</updated>
    <id>http://nick-aschenbach.github.io/blog/2014/07/18/generating-code-from-color-gradients</id>
    <content type="html"><![CDATA[I needed to incorporate smooth color transitions into a few projects I worked on several months ago. I wrote a 3D virtual terrain generator and wanted to color areas based on altitude. Higher elevations should be colored white (mountains), medium elevations should be colored green (grass) and lower elevations should be colored blue (water). A second application for these transitions was a series of particle systems (see video below). Changing particle color as a function of age allows effects like fading to black before a particle is removed. We will come back to particle systems later with a small demo at the end of this post.

{% youtube -urfsS1OpYo %}

To achieve color transitions in my programs, I figured that raster images with color gradients could be used. Programs like Adobe Photoshop or [GIMP](http://www.gimp.org/) are excellent tools to use for this purpose. As an aside, I also enjoy using the ColorZilla [Ultimate CSS Gradient Editor](http://www.colorzilla.com/gradient-editor/) for web projects. The problem I faced was getting red, blue and green values at each pixel along the gradient into code. 

<!-- more -->

I found quite a few libraries that could read images. ImageMagick is a powerful set of tools that enable reading and writing over 100 image formats. It has interfaces for a [number of languages](http://www.imagemagick.org/script/api.php). However, I was looking for something light-weight and self contained. 

I found an easy-to-use gem called [Chunky PNG](https://github.com/wvanbergen/chunky_png) that allows developers to read and write PNG files. Here is a snippet of code that shows how to open and read a PNG file in Ruby:

{% codeblock lang:rb %}
image = ChunkyPNG::Image.from_file(filename)
(0..image.dimension.width - 1).each do |x|
  (0..image.dimension.height - 1).each do |y|
    r = ChunkyPNG::Color.r(image[x, y])
    g = ChunkyPNG::Color.g(image[x, y])
    b = ChunkyPNG::Color.b(image[x, y])
  end
end
{% endcodeblock %}

While this gem is written in 100% Ruby, there is a gem extension library that speeds up Chunky called [Oily PNG](https://github.com/wvanbergen/oily_png). It optimizes some operations like encoding and decoding PNG files using native C code.

By taking PNG files as input and generating Java code as output, I found that I could save a lot of time and be more flexible with changing color transitions than if I had typed out the code by hand. This is easy to do:

{% codeblock lang:rb %}
def parse_image(filename)
  str = &#8220;public static Color gradient[] = {\n&#8221;
  image = ChunkyPNG::Image.from_file(filename)
  (0..image.dimension.width - 1).each do |x|
    r = ChunkyPNG::Color.r(image[x, 0])
    g = ChunkyPNG::Color.g(image[x, 0])
    b = ChunkyPNG::Color.b(image[x, 0])

    str += &#8221;  new Color(#{r}, #{g}, #{b}),\n&#8221;
  end
  str += &#8220;};&#8221;;
end
{% endcodeblock %}

I ended up generating about 40 images that I used in conjuncton with a small color gradient manager library in Java. The source code and generator are available on [Github](https://github.com/nick-aschenbach/code-generated-color-gradients). This example shows its use:

{% codeblock lang:java %}
ColorGradientManager cgm = new ColorGradientManager();

// List all available color gradients
System.out.println(cmg.getAllGradients());

// Select a gradient
cgm.select(&#8220;Land Sea&#8221;);

// Get the colors
for (Color c : cgm.gradient()) {
  System.out.print(c + &#8221; &#8220;);
}
{% endcodeblock %}

I decided to modify the generator a little bit to output code for Javascript and used it for a simple particle system simulation (available on [Github](https://github.com/nick-aschenbach/particle-system-color-gradients)). The color gradients can be changed out dynamically via the drop down box.

<iframe src="http://nick-aschenbach.github.io/assets/2014-07-18-generating-code-from-color-gradients/index.html" width="300" height="300"></iframe>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Maps, Lasers and Bamboo]]></title>
    <link href="http://nick-aschenbach.github.io/blog/2014/07/14/laser-etched-map-lazy-susan/"/>
    <updated>2014-07-14T22:41:29-07:00</updated>
    <id>http://nick-aschenbach.github.io/blog/2014/07/14/laser-etched-map-lazy-susan</id>
    <content type="html"><![CDATA[{% img right /images/colorado-mountains.jpg Snowy view towards Devil's Thumb Ranch %}

I spent a week over the winter at 8,500 feet in the mountains at Old Sky Valley Ranch near Tabernash, Colorado. It is an inspiring place and I undertook a project with fellow GIS developer Nicholas Hallahan (https://github.com/hallahan/ http://www.spatialdev.com/#about) to craft a gift for one of the ranch families.

The project involved quite a bit of GIS modeling and graphic design work. The spatial data was obtained from a variety of sources including the USGS National Map Viewer (http://viewer.nationalmap.gov/viewer/) where we found the contour data. We also obtained some of the road, hydrology, and cadastral (building footprints) data from the Grand County, CO GIS Department (http://co.grand.co.us/170/Digital-Data-Sets). Much of the fine grained details of this data were missing, such as some of the less develped roads and the precision of bends of the rivers and streams. This was achieved by hand digitizing from satellite imagery features that were missing in OpenStreetMap and then importing that final product. This part was a lot of fun, and in turn we got a new feeling and perspective of the geography of the area.

<!-- more -->

{% img right /images/colorado-mid-scale.jpg 350 auto Mid scale map %}

Once all of the source data was gathered, we used ArcGIS to filter out unwanted features and narrow down the area of the data to the extent of the map we wanted to produce. The work was further cleaned up in Adobe Illustrator.

Although ArcGIS is well equipped to edit vector data, it focuses on this data in a geographic context, and we needed to specify details in the final, printable vector format. This is where Adobe Illustrator comes in. Here we were able to carefully specify line widths as well as place labels for these lines. The most important part of this process was separating different features into different layers that would be cut (burned) with different settings and intensities by the laser.

Computer Numerical Controlled (CNC) machine tools are robots that help automate the process of making custom parts. They typically involve the use of computer aided design (CAD) and computer aided manufacturing (CAM) software tools. The main benefit of CNC tools is both that they automate the manufacturing process and are precise. In recent years CNC machines have become much more affordable. For example, small CNC routers that can cut wood, plastics and light metals are commercially available for a few thousand dollars (USD).

Creating the lazy susan involved several steps that included both a CNC router and CNC laser. Carbonized 3/4&#8221; thick three-ply natural bamboo plywood sheet was selected due to its light color and durability. A 17&#8221; diameter circle was cut out on a CNC router. We used ADX Portland (http://www.adxportland.com) for their CNC laser service (they have an 18&#8221; x 24&#8221; table). We were able to index the part on the laser table by first cutting a 17&#8221; diameter disk in a thin piece of cardboard. We could then locate the same-sized bamboo disk into the hole we cut in the cardboard.

CNC lasers can be configured to cut in either a vector or raster mode. Vectors cutting is best for lines and arcs that are precisely (aka mathematically) defined. Raster cutting is better for laser engraving and is ideal for filled areas and photographs. Our project required both raster and vector cut modes. The raster mode was used to fill in the lakes and the interiors of the letters. The vector mode was used for the contour lines and letter outlines. 

Map details look best when line weights are taken into consideration. While the laser typically cuts lines the same thickness in vector mode, we found that we could cut wider lines by defocusing the laser. This was achieved by setting the Z-home position roughly 1/8&#8221; or 3/16&#8221; above the part instead of on the part. Without this hack, we would have needed to do the burning in a &#8220;raster&#8221; mode in which the machine burns in a pixel-based mannor similiar to what you would see with an ink jet printer. This would have taken a very long time, and we were charged by the minute.

To finish up the part we went back to the CNC router to pocket it out for the lazy susan mechanism. The 9&#8221; turntable was obtained from Tap Plastics (http://www.tapplastics.com/). While the pocket diameter was 9-1/16&#8221;, the depth was slightly less than the height of the mechanism. This allows the turntable to support the bamboo piece up off of the table. Finally, mineral oil was used to finish the part and protect it. Mineral oil is both food safe, readily available (check your local drug store) and brings out color the natural bamboo.

{% img /images/lazy-susan-collage.jpg Finished lazy susan %}]]></content>
  </entry>
  
</feed>
